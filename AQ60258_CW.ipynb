{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "801d6d18",
      "metadata": {
        "id": "801d6d18"
      },
      "source": [
        "\n",
        "## Part I – Distributional Hypothesis\n",
        "\n",
        "| Sentence context clues | Correct word | Why? |\n",
        "|------------------------|--------------|------|\n",
        "| “piece of …”, “eat …”, “cut with knife”, “made from milk” | **cheese** | All four collocations are prototypical of *cheese*. *Cake* is not made from milk and *butter* is rarely “a piece of”. |\n",
        "| “parked in driveway”, “bought”, “drive fast”, “wash on weekends” | **car** | Fits every clue; motorcycles aren’t usually “parked in driveway & washed”. |\n",
        "| “read”, “enjoy before bed”, “has chapters & cover”, “borrow from library” | **book** | Only *book* unites all four contexts. |\n",
        "\n",
        "**Summary.** Under the Distributional Hypothesis, words appearing in similar contexts have related meanings. Matching each set of shared contexts selects **cheese**, **car**, and **book** as the correct completions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d040a4fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d040a4fb",
        "outputId": "749c5f31-1476-464f-c6d3-0529c409b437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 5‑fold Stratified CV ===\n",
            "LogReg   : 0.815 ± 0.075\n",
            "LinSVM   : 0.800 ± 0.045\n",
            "MultNB   : 0.777 ± 0.066\n",
            "RandForest: 0.792 ± 0.062\n",
            "\n",
            "=== Leave‑One‑Plot‑Out (GroupKFold) ===\n",
            "LogReg   : 0.804 ± 0.186\n",
            "LinSVM   : 0.805 ± 0.183\n",
            "MultNB   : 0.711 ± 0.142\n",
            "RandForest: 0.642 ± 0.118\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd, numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, GroupKFold, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/data_stories_one_shot.csv')\n",
        "\n",
        "# Map Stage to binary label: Stage 1 = Show (0), Stage 2/3 = Tell (1)\n",
        "df['Label'] = df['Stage'].apply(lambda x: 0 if x == 1 else 1)\n",
        "\n",
        "X = df['Sentence'].values\n",
        "y = df['Label'].values\n",
        "groups = df['Plot_Name'].values\n",
        "\n",
        "# TF‑IDF vectoriser with English stop‑words and alphabetic tokens only (≥2 chars)\n",
        "vectorizer = TfidfVectorizer(lowercase=True,\n",
        "                             stop_words='english',\n",
        "                             token_pattern=r'\\b[a-z]{2,}\\b')\n",
        "\n",
        "classifiers = {\n",
        "    'LogReg': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
        "    'LinSVM': LinearSVC(class_weight='balanced', random_state=42),\n",
        "    'MultNB': MultinomialNB(),\n",
        "    'RandForest': RandomForestClassifier(n_estimators=300,\n",
        "                                         class_weight='balanced',\n",
        "                                         random_state=42)\n",
        "}\n",
        "\n",
        "def eval_model(pipe, cv, **kwargs):\n",
        "    scores = cross_val_score(pipe, X, y, cv=cv, scoring='accuracy', **kwargs)\n",
        "    return scores.mean(), scores.std()\n",
        "\n",
        "# 5‑fold Stratified CV\n",
        "print(\"=== 5‑fold Stratified CV ===\")\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "for name, clf in classifiers.items():\n",
        "    pipe = make_pipeline(vectorizer, clf)\n",
        "    mean, std = eval_model(pipe, skf)\n",
        "    print(f\"{name:<9}: {mean:.3f} ± {std:.3f}\")\n",
        "\n",
        "# Leave‑One‑Plot‑Out\n",
        "print(\"\\n=== Leave‑One‑Plot‑Out (GroupKFold) ===\")\n",
        "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
        "for name, clf in classifiers.items():\n",
        "    pipe = make_pipeline(vectorizer, clf)\n",
        "    mean, std = eval_model(pipe, gkf, groups=groups)\n",
        "    print(f\"{name:<9}: {mean:.3f} ± {std:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "052b5c18",
      "metadata": {
        "id": "052b5c18"
      },
      "source": [
        "\n",
        "### Results & Discussion\n",
        "\n",
        "* Linear models (*Logistic Regression* and *Linear SVM*) consistently outperform Naïve Bayes and Random Forest on both CV schemes, echoing Figure 6 of the assignment paper.\n",
        "* The wider gap under **leave‑one‑plot‑out** highlights how some plots differ stylistically; linear models still generalise better.\n",
        "* For the optional “bonus”, try replacing the TF‑IDF step with Sentence‑BERT embeddings (`sentence-transformers` library) and re‑running the CV blocks—the code is modular to allow that swap.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}